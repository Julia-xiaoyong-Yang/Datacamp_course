############
NMT example
This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.

Some sample sentences are available on the sentences variable and are printed on the console.

Also, a pre-trained model is available on the model variable and you will use two custom functions to simplify some steps:

encode_sequences(): Change texts into sequence of numerical indexes and pad them.
translate_many(): Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.
For more details on the functions, use help(). The package pandas is loaded as pd.
############
# Transform text into sequence of indexes and pad
X = encode_sequences(sentences)

# Print the sequences of indexes
print(X)

# Translate the sentences
translated = translate_many(model, X)

# Create pandas DataFrame with original and translated
df = pd.DataFrame({'Original': sentences, 'Translated': translated})

# Print the DataFrame
print(df)

####################
Predict next character
In this exercise, you will code the function to predict the next character given a trained model. You will use the past 20 chars to predict the next one. You will learn how to train the model in the next lesson, as this step is integral before model training.

This is the initial step to create rules for generating sentences, paragraphs, short texts or other blocks of text as needed.

The variables n_vocab, chars_window and the dictionary index_to_char are already loaded in the environment. Also, the functions below are already created for you:

initialize_X(): Transforms the text input into a sequence of index numbers with the correct shape.
predict_next_char(): Gets the next character using the .predict() method of the model class and the index_to_char dictionary.
###############
def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):
  	# Initialize the X vector with zeros
    X = initialize_X(initial_text, chars_window, char_to_index)
    
    # Get next character using the model

#############
Generate sentence with context
In this exercise, you are going to experiment on a pre-trained model for text generation. The model is already loaded in the environment in the model variable, as well as the initialize_params() and get_next_token() functions.

This later uses the pre-trained model to predict the next character and return three variables: the next character next_char, the updated sentence res and the the shifted text seq that will be used to predict the next one.

You will define a function that receives a pre-trained model and a string that will be the start of the generated sentence as inputs. This is a good practice to generate text with context. The sentence limit of 100 characters is an example, you can use other limits (or even without limit) in your applications.
################
    next_char = predict_next_char(model, X, index_to_char)
	
    return next_char

# Define context sentence and print the generated text
initial_text = "I am not insane, "
print("Next character: {0}".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))

def generate_phrase(model, initial_text):
    # Initialize variables  
    res, seq, counter, next_char = initialize_params(initial_text)
    
    # Loop until stop conditions are met
    while counter < 100 and next_char != r'.':
      	# Get next char using the model and append to the sentence
        next_char, res, seq = get_next_token(model, res, seq)
        # Update the counter
        counter = counter + 1
    return res
  
# Create a phrase
print(generate_phrase(model, "I am not insane, "))

#############
Change the probability scale
In this exercise, you will see the difference in the resulted sentence when using different values of temperature to scale the probability distribution.

The function generate_phrase() is an adaptation of the function you created before and is already loaded in the environment. It receives the parameters model with the pre-trained model, initial_text with the context text and temperature that is the value to scale the softmax() function.
################
# Define the initial text
initial_text = "Spock and me "

# Define a vector with temperature values
temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]

# Loop over temperatures and generate phrases
for temperature in temperatures:
	# Generate a phrase
	phrase = generate_phrase(model, initial_text, temperature)
    
	# Print the phrase
	print('Temperature {0}: {1}'.format(temperature, phrase))

#############
Create vectors of sentences and next characters
This exercise aims to emphasize more the value of data preparation. You will use texts containing phrases of the character Sheldon from The Big Bang Theory TV show as input and will create vectors of sentence indexes and next characters that are needed before creating a text generation model.

The text is available in the sheldon variable, as well as the vocabulary (characters) on the vocabulary variable and the hyperparameters chars_window and step defined with values 20 and 3. This means that a sequence of 20 characters will be used to predict the next one, and the window will shift 3 characters on every iteration.

Also, the package pandas as pd is loaded in the environment.
###############
# Instantiate the vectors
sentences = []
next_chars = []
# Loop for every sentence
for sentence in sheldon.split('\n'):
    # Get 20 previous chars and next char; then shift by step
    for i in range(0, len(sentence) - chars_window, step):
        sentences.append(sentence[i:i + chars_window])
        next_chars.append(sentence[i + chars_window])

# Define a Data Frame with the vectors
df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})

# Print the initial rows
print(df.head())

##############
Preparing the data for training
In this exercise, you will continue to prepare the data to train the model. After creating the arrays of sentences and next characters, you need to transform them to numerical values that can be used on the model.

This step is necessary because the RNN models expect numbers only and not strings. You will create numerical arrays that have zeros or ones in the positions representing the characters present on the sentences. Ones (or True) represent the corresponding character is present, while zeros (or False) represent the absence of the character in that position of the sentence.

The variables sentences, next_char, n_vocab, chars_window, num_seqs (number of sentences in the training data) are already loaded in the environment, as well as numpy as np.
###############
# Instantiate the variables with zeros
numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)
numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)

# Loop for every sentence
for i, sentence in enumerate(sentences):
  # Loop for every character in sentence
  for t, char in enumerate(sentence):
    # Set position of the character to 1
    numerical_sentences[i, t, char_to_index[char]] = 1
    # Set next character to 1
    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1

# Print the first position of each
print(numerical_sentences[0], numerical_next_chars[0], sep="\n")

########
Creating the text generation model
In this exercise, you will define a text generation model using Keras.

The variables n_vocab containing the vocabulary size and input_shape containing the shape of the data used for training are already loaded in the environment. Also, the weights of a pre-trained model is available on file model_weights.h5. The model was trained with 40 epochs on the training data. Recap that to train a model in Keras, you just use the method .fit() on the training data (X, y), and the parameter epochs. For example:

model.fit(X_train, y_train, epochs=40)
##############
# Instantiate the model
model = Sequential(name="LSTM model")

# Add two LSTM layers
model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name="Input_layer"))
model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name="LSTM_hidden"))

# Add the output layer
model.add(Dense(n_vocab, activation='softmax', name="Output_layer"))
# Compile and load weights
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.load_weights('model_weights.h5')
# Summary
model.summary()

###########
Preparing the input text
You have seen in the video how to prepare the input and output texts. This exercise aims to show a common practice that is to use the maximum length of the sentences to pad all of them, this way no information will be lost.

Since the RNN models need the inputs to have the same size, this is a way to pad all sentences and just add zeros to the smaller sentences, without cutting the larger ones.

Also, you will use words instead of characters to represent the tokens, this is a common approach for NMT models.

The Portuguese texts are loaded on the pt_sentences variable and a fitted tokenizer on the input_tokenizer variable.
#############
# Get maximum length of the sentences
pt_length = max([len(sentence.split(' ')) for sentence in pt_sentences])

# Transform text to sequence of numerical indexes
X = input_tokenizer.texts_to_sequences(pt_sentences)

# Pad the sequences
X = pad_sequences(X, maxlen=pt_length, padding='post')

# Print first sentence
print(pt_sentences[0])

# Print transformed sentence
print(X[0])

##########
Preparing the output text
In this exercise, you will prepare the output texts to be used on the translation model. Apart from transforming the text to sequences of indexes, you also need to one-hot encode each index.

The English texts are loaded on the en_sentences variable, the fitted tokenizer on the output_tokenizer variable and the English vocabulary size on en_vocab_size.

Also, a function to perform the first steps of transforming the output language (transformation of texts into sequence of indexes) is already created. The function is loaded on the environment as transform_text_to_sequences() and has two parameters: sentences that expect a list of sentences in English and tokenizer that expects a fitted Tokenizer object from the keras.preprocessing.text module.

numpy is loaded as np.
#############
# Initialize the variable
Y = transform_text_to_sequences(en_sentences, output_tokenizer)

# Temporary list
ylist = list()
for sequence in Y:
  	# One-hot encode sentence and append to list
    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))

# Update the variable
Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)

# Print the raw sentence and its transformed version
print("Raw sentence: {0}\nTransformed: {1}".format(en_sentences[0], Y[0]))

#########
Translate Portuguese to English
This is the last exercise of the course, congratulations on getting here!

You will learn how to use NMT models for making translations.

A model that encodes Portuguese small phrases and decodes them into English small phrases was pre-trained and is loaded in the model variable.

Also, the function predict_one() is already loaded, use help() for details and the dataset is available on the test (raw text) and X_test (tokenized) variables.

You will define a function to translate a list of sentences. In the parameters, sentences is a list of phrases to be translated, index_to_word is a dict containing numerical indexes as keys and words as values for the English language, loaded in the en_index_to_word variable.

The model summary has been printed for your consideration.
##############
# Function to predict many phrases
def predict_many(model, sentences, index_to_word, raw_dataset):
    for i, sentence in enumerate(sentences):
        # Translate the Portuguese sentence
        translation = predict_one(model, sentence, index_to_word)
        
        # Get the raw Portuguese and English sentences
        raw_target, raw_src = raw_dataset[i]
        
        # Print the correct Portuguese and English sentences and the predicted
        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))

predict_many(model, X_test[0:10], en_index_to_word, test)
